"""This module contains the logic for the Retrieval-Augmented Generation (RAG) system."""
import chromadb
from backend.prompts import RAG_PROMPT_TEMPLATE
from backend.services.llm import client, CHAT_MODEL

# Initialize the Chroma DB client and the embedding model
db_client = chromadb.HttpClient(host="chroma", port=8000)
embedding_model = "models/embedding-001"
collection = db_client.get_or_create_collection(name="dnd_lore")


def ask_rag_question(prompt: str) -> str:
    """
    Answers a question using RAG by retrieving relevant context from Chroma DB.

    Args:
        prompt: The user's question.

    Returns:
        The answer generated by the LLM based on the retrieved context.
    """
    # 1. Embed the user's prompt
    prompt_embedding = client.models.embed_content(
        model=embedding_model,
        contents=[prompt],
    ).embeddings[0].values

    # 2. Query the vector database for relevant context
    results = collection.query(
        query_embeddings=[prompt_embedding],
        n_results=3,
    )
    retrieved_docs = results.get("documents", [[]])[0]
    context = "\n".join(retrieved_docs)

    # 3. Construct a new prompt with the retrieved context
    rag_prompt = RAG_PROMPT_TEMPLATE.format(context=context, prompt=prompt)

    # 4. Call the LLM with the augmented prompt
    response = client.models.generate_content(
        model=CHAT_MODEL,
        contents=rag_prompt,
    )
    return response.text
